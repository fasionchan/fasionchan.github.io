<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tcp | fasionchan]]></title>
  <link href="http://blog.fasionchan.com/blog/categories/tcp/atom.xml" rel="self"/>
  <link href="http://blog.fasionchan.com/"/>
  <updated>2017-10-15T20:23:37+08:00</updated>
  <id>http://blog.fasionchan.com/</id>
  <author>
    <name><![CDATA[陈彦霏]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[译文]深入理解Linux TCP Backlog]]></title>
    <link href="http://blog.fasionchan.com/blog/2016/11/07/yi-wen-shen-ru-li-jie-linux-tcp-backlog/"/>
    <updated>2016-11-07T21:57:09+08:00</updated>
    <id>http://blog.fasionchan.com/blog/2016/11/07/yi-wen-shen-ru-li-jie-linux-tcp-backlog</id>
    <content type="html"><![CDATA[<p>当应用程序调用<code>listen</code>系统调用让一个<code>socket</code>进入<code>LISTEN</code>状态时，需要指定一个参数<code>backlog</code>。这个<code>backlog</code>参数经常被描述为，新连接队列的长度限制。</p>

<p><img src="http://upload-images.jianshu.io/upload_images/2740477-d1ebcbbc58e3b9f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="tcp-state-diagram.png" /></p>

<!--more-->


<p>由于<code>TCP</code>建立连接使用3次握手，因此，一个新连接在到达<code>ESTABLISHED</code>状态可以被<code>accept</code>系统调用返回给应用程序前，必须经过一个中间状态<code>SYN RECEIVED</code>(见上图)。这意味着，<code>TCP/IP</code>协议栈在实现<code>backlog</code>队列时，有两种不同的选择：</p>

<ol>
<li><p>仅使用一个队列，队列规模由<code>listen</code>系统调用<code>backlog</code>参数指定。当协议栈收到一个<code>SYN</code>包时，响应<code>SYN/ACK</code>包并且将连接加进该队列。当相应的<code>ACK</code>响应包收到后，连接变为<code>ESTABLISHED</code>状态，可以向应用程序返回。这意味着队列里的连接可以有两种不同的状态：<code>SEND RECEIVED</code>和<code>ESTABLISHED</code>。只有后一种连接才能被<code>accept</code>系统调用返回给应用程序。</p></li>
<li><p>使用两个队列——<code>SYN</code>队列(待完成连接队列)和<code>accept</code>队列(已完成连接队列)。状态为<code>SYN RECEIVED</code>的连接进入<code>SYN</code>队列，后续当状态变更为<code>ESTABLISHED</code>时移到<code>accept</code>队列(即收到3次握手中最后一个<code>ACK</code>包)。顾名思义，<code>accept</code>系统调用就只是简单地从<code>accept</code>队列消费新连接。在这种情况下，<code>listen</code>系统调用<code>backlog</code>参数决定<code>accept</code>队列的最大规模。</p></li>
</ol>


<p>历史上，起源于<code>BSD</code>的<code>TCP</code>实现使用第一种方法。这个方案意味着，但<code>backlog</code>限制达到，系统将停止对<code>SYN</code>包响应<code>SYN/ACK</code>包。通常，协议栈只是丢弃<code>SYN</code>包(而不是回一个<code>RST</code>包)以便客户端可以重试(而不是异常退出)。</p>

<p><code>TCP/IP详解 卷3</code>第<code>14.5</code>节中有提到这一点。书中作者提到，<code>BSD</code>实现虽然使用了两个独立的队列，但是行为跟使用一个队列并没什么区别。</p>

<p>在<code>Linux</code>上，情况有所不同，情况<code>listen</code>系统调用<code>man</code>文档页：</p>

<blockquote><p>The  behavior  of  the  backlog  argument on TCP sockets changed with Linux 2.2.  Now it specifies the queue length for completely established sockets waiting to be accepted, instead of the number of incomplete connection requests.  The maximum length of the queue for incomplete sockets can be set using /proc/sys/net/ipv4/tcp_max_syn_backlog.  When syncookies are enabled there is no logical maximum length and this setting is ignored.</p>

<p>意思是，<code>backlog</code>参数的行为在<code>Linux</code>2.2之后有所改变。现在，它指定了等待<code>accept</code>系统调用的已建立连接队列的长度，而不是待完成连接请求数。待完成连接队列长度由<code>/proc/sys/net/ipv4/tcp_max_syn_backlog</code>指定；在<code>syncookies</code>启用的情况下，逻辑上没有最大值限制，这个设置便被忽略。</p></blockquote>

<p>也就是说，当前版本的<code>Linux</code>实现了第二种方案，使用两个队列——一个<code>SYN</code>队列，长度系统级别可设置以及一个<code>accept</code>队列长度由应用程序指定。</p>

<p>现在，一个需要考虑的问题是在<code>accept</code>队列已满而一个已完成新连接需要用<code>SYN</code>队列移动到<code>accept</code>队列(收到3次握手中最后一个<code>ACK</code>包)，这个实现方案是什么行为。这种情况下，由<code>net/ipv4/tcp_minisocks.c</code>中<code>tcp_check_req</code>函数处理：</p>

<pre><code>    child = inet_csk(sk)-&gt;icsk_af_ops-&gt;syn_recv_sock(sk, skb, req, NULL);
    if (child == NULL)
        goto listen_overflow;
</code></pre>

<p>对于<code>IPv4</code>，第一行代码实际上调用的是<code>net/ipv4/tcp_ipv4.c</code>中的<code>tcp_v4_syn_recv_sock</code>函数，代码如下：</p>

<pre><code>    if (sk_acceptq_is_full(sk))
        goto exit_overflow;
</code></pre>

<p>可以看到，这里会检查<code>accept</code>队列的长度。如果队列已满，跳到<code>exit_overflow</code>标签执行一些清理工作、更新<code>/proc/net/netstat</code>中的统计项<code>ListenOverflows</code>和<code>ListenDrops</code>，最后返回<code>NULL</code>。这会触发<code>tcp_check_req</code>函数跳到<code>listen_overflow</code>标签执行代码。</p>

<pre><code>listen_overflow:
    if (!sysctl_tcp_abort_on_overflow) {
        inet_rsk(req)-&gt;acked = 1;
        return NULL;
    }
</code></pre>

<p>很显然，除非<code>/proc/sys/net/ipv4/tcp_abort_on_overflow</code>被设置为<code>1</code>(这种情况下发送一个<code>RST</code>包)，实现什么都没做。</p>

<p>总结一下：<code>Linux</code>内核协议栈在收到3次握手最后一个<code>ACK</code>包，确认一个新连接已完成，而<code>accept</code>队列已满的情况下，会忽略这个包。一开始您可能会对此感到奇怪——别忘了<code>SYN RECEIVED</code>状态下有一个计时器实现：如果<code>ACK</code>包没有收到(或者是我们讨论的忽略)，协议栈会重发<code>SYN/ACK</code>包(重试次数由<code>/proc/sys/net/ipv4/tcp_synack_retries</code>决定)。</p>

<p>看以下抓包结果就非常明显——一个客户正尝试连接一个已经达到其最大<code>backlog</code>的<code>socket</code>：</p>

<pre><code>  0.000  127.0.0.1 -&gt; 127.0.0.1  TCP 74 53302 &gt; 9999 [SYN] Seq=0 Len=0
  0.000  127.0.0.1 -&gt; 127.0.0.1  TCP 74 9999 &gt; 53302 [SYN, ACK] Seq=0 Ack=1 Len=0
  0.000  127.0.0.1 -&gt; 127.0.0.1  TCP 66 53302 &gt; 9999 [ACK] Seq=1 Ack=1 Len=0
  0.000  127.0.0.1 -&gt; 127.0.0.1  TCP 71 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
  0.207  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
  0.623  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
  1.199  127.0.0.1 -&gt; 127.0.0.1  TCP 74 9999 &gt; 53302 [SYN, ACK] Seq=0 Ack=1 Len=0
  1.199  127.0.0.1 -&gt; 127.0.0.1  TCP 66 [TCP Dup ACK 6#1] 53302 &gt; 9999 [ACK] Seq=6 Ack=1 Len=0
  1.455  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
  3.123  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
  3.399  127.0.0.1 -&gt; 127.0.0.1  TCP 74 9999 &gt; 53302 [SYN, ACK] Seq=0 Ack=1 Len=0
  3.399  127.0.0.1 -&gt; 127.0.0.1  TCP 66 [TCP Dup ACK 10#1] 53302 &gt; 9999 [ACK] Seq=6 Ack=1 Len=0
  6.459  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
  7.599  127.0.0.1 -&gt; 127.0.0.1  TCP 74 9999 &gt; 53302 [SYN, ACK] Seq=0 Ack=1 Len=0
  7.599  127.0.0.1 -&gt; 127.0.0.1  TCP 66 [TCP Dup ACK 13#1] 53302 &gt; 9999 [ACK] Seq=6 Ack=1 Len=0
 13.131  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
 15.599  127.0.0.1 -&gt; 127.0.0.1  TCP 74 9999 &gt; 53302 [SYN, ACK] Seq=0 Ack=1 Len=0
 15.599  127.0.0.1 -&gt; 127.0.0.1  TCP 66 [TCP Dup ACK 16#1] 53302 &gt; 9999 [ACK] Seq=6 Ack=1 Len=0
 26.491  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
 31.599  127.0.0.1 -&gt; 127.0.0.1  TCP 74 9999 &gt; 53302 [SYN, ACK] Seq=0 Ack=1 Len=0
 31.599  127.0.0.1 -&gt; 127.0.0.1  TCP 66 [TCP Dup ACK 19#1] 53302 &gt; 9999 [ACK] Seq=6 Ack=1 Len=0
 53.179  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
106.491  127.0.0.1 -&gt; 127.0.0.1  TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq=1 Ack=1 Len=5
106.491  127.0.0.1 -&gt; 127.0.0.1  TCP 54 9999 &gt; 53302 [RST] Seq=1 Len=0
</code></pre>

<p>由于客户端的<code>TCP</code>实现在收到多个<code>SYN/ACK</code>包时，认为<code>ACK</code>包已经丢失了并且重传它。如果在<code>SYN/ACK</code>重试次数达到限制前，服务端应用从<code>accept</code>队列接收连接，使得<code>backlog</code>减少，那么协议栈会处理这些重传的<code>ACK</code>包，将连接状态从<code>SYN RECEIVED</code>变更到<code>ESTABLISHED</code>并且将其加入<code>accept</code>队列。否则，正如以上包跟踪所示，客户读会收到一个<code>RST</code>包宣告连接失败。</p>

<p>在客户端看来，第一次收到<code>SYN/ACK</code>包之后，连接就会进入<code>ESTABLISHED</code>状态。如果这时客户端首先开始发送数据，那么数据也会被重传。好在<code>TCP</code>有慢启动机制，在服务端还没进入<code>ESTABLISHED</code>之前，客户端能发送的数据非常有限。</p>

<p>相反，如果客户端一开始就在等待服务端，而服务端<code>backlog</code>没能减少，那么最后的结果是连接在客户端看来是<code>ESTABLISHED</code>状态，但在服务端看来是<code>CLOSED</code>状态。这也就是所谓的半开连接。</p>

<p>有一点还没讨论的是：<code>man listen</code>中提到每次收到新<code>SYN</code>包，内核往<code>SYN</code>队列追加一个新连接(除非该队列已满)。事实并非如此，<code>net/ipv4/tcp_ipv4.c</code>中<code>tcp_v4_conn_request</code>函数负责处理<code>SYN</code>包，请看以下代码：</p>

<pre><code>    if (sk_acceptq_is_full(sk) &amp;&amp; inet_csk_reqsk_queue_young(sk) &gt; 1) {
        NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
        goto drop;
    }
</code></pre>

<p>可以看到，在<code>accept</code>队列已满的情况下，内核会强制限制<code>SYN</code>包的接收速率。如果有大量<code>SYN</code>包待处理，它们其中的一些会被丢弃。这样看来，就完全依靠客户端重传<code>SYN</code>包了，这种行为跟<code>BSD</code>实现一样。</p>

<p>下结论前，需要再研究以下<code>Linux</code>这种实现方式跟<code>BSD</code>相比有什么优势。<code>Stevens</code>是这样说的：</p>

<blockquote><p>在<code>accept</code>队列已满或者<code>SYN</code>队列已满的情况下，<code>backlog</code>会达到限制。第一种情况经常发生在服务器或者服务器进程非常繁忙的情况下，进程没法足够快地调用<code>accept</code>系统调用从中取出已完成连接。后者是<code>HTTP</code>服务器经常面临的问题，在服务端客户端往返时间非常长的时候(相对于连接到达速率)，因为新<code>SYN</code>包在往返时间内都会占据一个连接对象。</p>

<p>大多数情况下<code>accept</code>队列都是空的，因为一旦有一个新连接进入队列，阻塞等待的<code>accept</code>系统调用将返回，然后连接从队列中取出。</p></blockquote>

<p><code>Stevens</code>建议的解决方案是简单地调大<code>backlog</code>。但有个问题是，应用程序在调优<code>backlog</code>参数时，不仅需要考虑自身对新连接的处理逻辑，还需要考虑网络状况，包括往返时间等。Linux实现实际上分成两部分：应用程序只负责调解<code>backlog</code>参数，确保<code>accept</code>调用足够快以免<code>accept</code>队列被塞满；系统管理员则根据网络状况调节<code>/proc/sys/net/ipv4/tcp_max_syn_backlog</code>，各司其职。</p>

<p>本文译自：<a href="http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html">How TCP backlog works in Linux</a>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[谈谈TCP TIME_WAIT状态]]></title>
    <link href="http://blog.fasionchan.com/blog/2015/06/05/tan-tan-tcp-time-wait-zhuang-tai/"/>
    <updated>2015-06-05T21:16:18+08:00</updated>
    <id>http://blog.fasionchan.com/blog/2015/06/05/tan-tan-tcp-time-wait-zhuang-tai</id>
    <content type="html"><![CDATA[<p>学过<code>TCP</code>协议的童鞋对以下状态变迁图应该不陌生，本文着重讨论<code>TIME_WAIT</code>状态。</p>

<p><img src="/images/2015/tcp-state-machine.gif"></p>

<p>从图中可以清楚看到：主动关闭的一端最终会进入<code>TIME_WAIT</code>状态，并维持<code>2*MSL</code>时间，一般情况下为<code>240</code>秒。
那么，为什么需要<code>TIME_WAIT</code>状态呢？为什么需要持续<code>2*MSL</code>时长呢？其中有什么特别的考虑吗？</p>

<h2>作用</h2>

<p>从上图可以看到，进入<code>TIME_WAIT</code>状态前，收到对端<code>FIN</code>包并且回复<code>ACK</code>包。
网络是不可靠的，也就是说最后这个<code>ACK</code>包可能被中间链路丢掉了。
对端协议栈在还有收到<code>ACK</code>包的情况下，将重传<code>FIN</code>包，本端需要对<code>FIN</code>包进行相应。
如果此时连接资源已经回收，系统只能响应<code>RST</code>包，这时对端就会认为是异常退出。
因此，保留<code>TIME_WAIT</code>状态套接字并维持一段时间，就可以正确响应对端重传的<code>FIN</code>包，让<code>TCP</code>连接优雅关闭。</p>

<p>另一场景是，对端可能还有<code>FIN</code>包还在网络逗留(延迟)。
如果新连接复用旧连接的所有要素(包括地址-端口对、<code>TCP</code>序列号)，旧连接迟到的<code>FIN</code>将<strong>终止新连接</strong>！
当然了，这个场景出现的概率比较低，但还是存在。</p>

<h2>问题</h2>

<p>当一个系统主动关闭大量<code>TCP</code>连接时，由于<code>2MSL</code>存在，将产生大量的<code>TIME_WAIT</code>状态连接。
<code>TCP</code>连接需要消耗一定的系统资源，因此，极端情况下将导致活跃连接<strong>响应速度下降</strong>甚至<strong>停止服务</strong>。</p>

<p>发起主动关闭的一端一般是客户端，这时候意味着服务端可以高枕无忧呢？</p>

<p>肯定不是的。服务端也有很多场景会发起主动关闭，最常见的应该是类似<code>nginx</code>之类的反向代理了。</p>

<p><img src="/images/2015/nginx-forward.png"></p>

<p>如上图，<code>Web</code>服务经常按照业务逻辑进行划分并独立部署，前端采用<code>Nginx</code>做统一接入以及负载均衡。
对<code>Web</code>服务器来说(黄色部分)，<code>Nginx</code>服务器为客户端。在<code>Nginx</code>转发请求完成后，主动关闭不可避免。
这样，当系统存在高并发短连接请求时，<code>Nginx</code>服务器上有大量<code>TIME WAIT</code>状态连接存在也就不奇怪了。</p>

<h2>解决方案</h2>

<h3>开启<code>SYN Cookies</code></h3>

<p>当<code>SYN</code>队列溢出时，采用<code>Cookie</code>来处理，继续接受新连接。
<code>SYN Cookie</code>可以在一定程度防范<code>SYN Flood</code>攻击，详情请参看这里。
配置选项为<code>net.ipv4.tcp_syncookies</code>，可以采用<code>sysctl</code>或者<code>proc</code>伪文件系统两种方式设置。</p>

<p><strong><code>sysctl</code>方式</strong>。编辑文件<code>/etc/sysctl.conf</code>，调整或增加以下行：</p>

<pre><code>net.ipv4.tcp_syncookies = 1
</code></pre>

<p>调整完成后，运行命令<code>sysctl -p</code>即可生效。</p>

<p><strong><code>proc</code>伪文件系统方式</strong>。</p>

<pre><code># 查看当前值
$ cat /proc/sys/net/ipv4/tcp_syncookies
0

# 开启
$ echo 1 &gt; /proc/sys/net/ipv4/tcp_syncookies

# 关闭
$ echo 0 &gt; /proc/sys/net/ipv4/tcp_syncookies
</code></pre>

<h3>开启TIME_WAIT重用</h3>

<p>开启<code>net.ipv4.tcp_tw_reuse</code>选项，将允许将<code>TIME_WAIT</code>状态<code>socket</code>重新用于新的<code>TCP</code>连接。
设置方式也可用通过<code>sysctl</code>和<code>proc</code>伪文件系统方式，请参考上节，下文不在赘述。</p>

<h3>开启TIME_WAIT快速回收</h3>

<p>开启<code>net.ipv4.tcp_tw_recycle</code>选项，快速回收处于<code>TIME_WAIT</code>状态的<code>socket</code>。</p>

<h3>修改FIN超时时间</h3>

<p>修改<code>net.ipv4.tcp_fin_timeout</code>选项。</p>

<h3>扩大外连端口范围</h3>

<p>修改<code>net.ipv4.ip_local_port_range</code>选项。这个选项表示向外连接的端口范围。
当然了，扩大范围只能缓解问题，无法彻底解决，另外端口也存在最大值<code>65000</code>左右。</p>

<p><code>net.ipv4.tcp_max_syn_backlog</code>，<code>net.ipv4.tcp_max_tw_buckets</code>，<code>net.ipv4.tcp_keepalive_time</code>。</p>

<h3>设置SO_LINGER选项</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP设计精髓]]></title>
    <link href="http://blog.fasionchan.com/blog/2012/09/06/tcp-she-ji-jing-sui/"/>
    <updated>2012-09-06T13:16:57+08:00</updated>
    <id>http://blog.fasionchan.com/blog/2012/09/06/tcp-she-ji-jing-sui</id>
    <content type="html"><![CDATA[<p><code>TCP</code>是在不可靠的网络层之上，实现的一种面向连接的可靠的传输层协议。
为了实现可靠性并尽量优化传输效率，<code>TCP</code>作者们考虑到方方面面，并设计了一系列巧妙的算法。</p>

<h2>纳格算法</h2>

<p>纳格算法(<code>Nagle's algoritm</code>)，主要思想是以减少发送量来增进<code>TCP/IP</code>网络的性能。</p>

<p>先来看一个场景——<strong>小数据包问题</strong>：
在远程终端操作中(<code>telnet</code>)，客户端所输入的每一个字符源源不断发送给服务端，服务端再根据输入内容进行回传显示。
这时，有大量的小数据包产生，很多都是只有一个字节(或者若干个字节)。
众所周知，<code>TCP</code>数据包头部就有<code>40</code>字节(<code>TCP</code>和<code>IPv4</code>各占<code>20</code>字节)。
这意味着，<code>41</code>字节的数据包只有<code>1</code>字节的有用信息，效率何等低下！
如果<code>TCP</code>立即发送应用产生的小数据包，那么慢速网络或者拥塞的环境下，情况将变得更糟糕。</p>

<p>那么，有什么办法可以解决这个问题吗？显而易见的做法是，对待发送数据进行合并之后再行提交。
这就是纳格算法的主要思想：
如果上次提交的数据还未收到对端确认，先持续缓冲数据包，直到一定大小才提交；
收到确认后，马上发送缓冲区中的数据包，不管数量是多少。</p>

<pre><code>if 缓冲区非空
    if 窗口大小 &gt;= MSS and 缓冲数据 &gt;= MSS
        理解发送MSS大小的分组
    else
        if 还有数据待确认
            继续缓冲数据(延迟发送)
        else
            理解发送数据
</code></pre>

<h2>捎带确认</h2>

<p><code>TCP</code>连接的两端进行通信，<code>A</code>给<code>B</code>发送一段数据，<code>B</code>需要确认数据收到(<code>ACK</code>)。
<code>B</code>每次收到数据立即回复<code>ACK</code>包，代价有点大，<code>40</code>个字节的<code>TCP</code>分组就为了告诉对方，”我已经收到数据了！“。
如果<code>B</code>刚好有数据需要发送，那么将<code>ACK</code>放在数据分组里(其实<code>ACK</code>只是一个标志位)捎带过去岂不更好？
因此，将<code>ACK</code>延迟到需要发送数据时才发送出去，可以<strong>零成本</strong>完成数据确认。</p>

<p>如果<code>B</code>不发送数据，那该咋整呢？长时间不回复<code>ACK</code>，对端该重传数据了，那更糟！
事实上，<code>TCP</code>会启动一个定时器，大概是<code>200ms</code>。
在这段时间内，如果有新数据分组要发送，那<code>ACK</code>刚好借这趟便车。
否则，<code>TCP</code>会回复一个独立的<code>ACK</code>包，这也是没有办法的事情了。
当然了，定时器选定也比较考究，需要足够长以便等到数据分组便车，但又不能太长导致对端重传数据。</p>
]]></content>
  </entry>
  
</feed>
